{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialization & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Packages used\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import pydicom\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras import backend as K\n",
    "\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "import skimage.transform as resize\n",
    "\n",
    "from keras import regularizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Flatten, MaxPool2D, Dropout, BatchNormalization\n",
    "\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rootdir = ('C:/Users/Job/Anaconda3/envs/TensorFlow-GPU/Pattern_Recognition/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data is loaded in and immediatly split into train/test sets, the labels are split aswell.\n",
    "## Loading in the mass data\n",
    "\n",
    "combined = np.load(rootdir + 'train_x.npy')\n",
    "combined_labels = np.load(rootdir + 'train_y.npy')\n",
    "\n",
    "combinedtest = np.load(rootdir + 'test_x.npy')\n",
    "combinedtest_labels = np.load(rootdir + 'test_y.npy')\n",
    "\n",
    "\n",
    "## Loading in the calc data\n",
    "combinedcalc = np.load(rootdir + 'calctrain_x.npy')\n",
    "combinedcalc_labels = np.load(rootdir + 'calctrain_y.npy')\n",
    "\n",
    "combinedtestcalc = np.load(rootdir + 'calctest_x.npy')\n",
    "combinedtestcalc_labels = np.load(rootdir + 'calctest_y.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Both datasets are combined to form a larger singular dataset.\n",
    "\n",
    "combinedtest = np.concatenate((combinedtest, combinedtestcalc))\n",
    "combinedtest_labels = np.concatenate((combinedtest_labels, combinedtestcalc_labels))\n",
    "\n",
    "combined = np.concatenate((combined, combinedcalc))\n",
    "combined_labels = np.concatenate((combined_labels, combinedcalc_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing the input to [-2,0]\n",
    "\n",
    "combined = np.subtract(combined, 1.0) \n",
    "combined = np.multiply(combined, 2.0) \n",
    "combinedtest = np.subtract(combinedtest, 1.0) \n",
    "combinedtest = np.multiply(combinedtest, 2.0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Randomizing the input, the respective order is maintained.\n",
    "\n",
    "randomize = np.arange(len(combined))\n",
    "np.random.shuffle(randomize)\n",
    "combined = combined[randomize]\n",
    "combined_labels = combined_labels[randomize]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################\n",
    "###############################################\n",
    "###############################################\n",
    "###############################################\n",
    "###############################################\n",
    "###############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CNN transfer learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ensuring that starting model is empty\n",
    "model = None\n",
    "tf.keras.backend.clear_session()\n",
    "gc.collect()\n",
    "\n",
    "## Defining the model as a sequential model\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Load the inceptionV3 model\n",
    "pretrained = InceptionV3(include_top=False, weights = 'imagenet', input_shape=(299,299,3))\n",
    "\n",
    "## Freezing blocks of layers, explained further in report\n",
    "for layer in pretrained.layers[:279]:\n",
    "    layer.trainable = False\n",
    "for layer in pretrained.layers[279:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "model.add(pretrained)\n",
    "\n",
    "# Summarize the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Adding our own layers\n",
    "\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(2, activation='softmax', kernel_regularizer=regularizers.l2(0.1)))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compiling the model with the predetermined learning rate\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n",
    "              loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "              metrics=['sparse_categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Defining a callback that checks whetether the validation accuracy has improved or not. Saving only max results.\n",
    "\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(rootdir + 'maxaccuracymodel.h5', monitor='val_sparse_categorical_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "## Defining a multiplier for the data augmentation, the batch size and the amount of images to be generated\n",
    "Mlt = 1 \n",
    "BS = 64\n",
    "steps_epoch = round(len(combined[0:2289])/BS)\n",
    "\n",
    "## Online imageaugmentor\n",
    "datagen = ImageDataGenerator(\n",
    "                rotation_range=20*Mlt,        \n",
    "                width_shift_range=0.1*Mlt,\n",
    "                height_shift_range=0.1*Mlt,\n",
    "                shear_range=0.35*Mlt,        \n",
    "                fill_mode='wrap', \n",
    "                horizontal_flip=True,\n",
    "                vertical_flip=True)\n",
    "                \n",
    "\n",
    "## Iterator that augments images on the fly, only on training data \n",
    "iterator_train = datagen.flow(combined[0:2289], combined_labels[0:2289], batch_size=BS)\n",
    "\n",
    "## Fitting the model, train/val split is: (0:2289 and 2289:2861)\n",
    "history = model.fit_generator(iterator_train, steps_per_epoch=steps_epoch, epochs = 3, callbacks=[checkpoint], \n",
    "                    validation_data=(combined[2289:2861],combined_labels[2289:2861]))\n",
    "\n",
    "## Note, a for loop has been used to train models on multiple combinations of learning and dropout rates. \n",
    "## This has been removed to make the code more readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################\n",
    "###############################################\n",
    "###############################################\n",
    "###############################################\n",
    "###############################################\n",
    "###############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading respective saved weights, or skip this part and just use the recently trained weights\n",
    "\n",
    "model.load_weights('finalmodelaccl1l2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Predicting model classes\n",
    "\n",
    "Predictions = model.predict_classes(combinedtest)\n",
    "Actual = combinedtest_labels\n",
    "\n",
    "## Showing the confusion matrix\n",
    "y_actu = pd.Series(Actual, name='Actual')\n",
    "y_pred = pd.Series(Predictions, name='Predicted')\n",
    "df_confusion = pd.crosstab(y_actu, y_pred)\n",
    "\n",
    "df_confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plotting the accuracy of the train and validation set\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.plot(history.history['sparse_categorical_accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_sparse_categorical_accuracy'], label = 'val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0.5, 1])\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Accuracy with data augmentation')\n",
    "\n",
    "## Saving the figure\n",
    "#plt.savefig(rootdir + 'output/acc248.png', dpi=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plotting the loss\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label = 'val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.ylim([0.0, 1.6])\n",
    "plt.title('Loss with data augmentation')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "#plt.savefig(rootdir + 'output/1acc248.png', dpi=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Plotting distribution of the data labels\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.hist(combined_labels, bins =[-0.25,0.25,0.75,1.25], label='vertical')\n",
    "plt.style.use('ggplot')\n",
    "plt.xlabel('Label')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Label Distribution of train/validation set')\n",
    "plt.xlim(-0.5, 1.5)\n",
    "plt.ylim(0, 2000)\n",
    "\n",
    "plt.xticks(np.arange(min(combined_labels), max(combined_labels)+1, 1.0))\n",
    "\n",
    "\n",
    "#plt.savefig(rootdir + 'output/LabelDistTrainVal.png', dpi=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plotting distribution of the data pixel values\n",
    "\n",
    "distreshape = combined.reshape(combined.size)\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.hist(distreshape, bins=256, range=(-2, 0), color='g')\n",
    "plt.xlabel('Normalised pixel value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution test set')\n",
    "plt.xlim(-2.1,0.1)\n",
    "plt.ylim(0, 8500000)\n",
    "\n",
    "#plt.savefig(rootdir + 'output/Distribution test.png', dpi=100)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-GPU",
   "language": "python",
   "name": "tensorflow-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
